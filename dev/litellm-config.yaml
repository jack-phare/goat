# LiteLLM proxy config for goat development.
# Mirrors the agent-hub production config so the same models and env vars work.
# Models are only available if the corresponding API key is set in .env.
# See: https://docs.litellm.ai/docs/proxy/configs

model_list:
  # Azure OpenAI (primary chat models — matches agent-hub)
  - model_name: gpt-4o-mini
    litellm_params:
      model: azure/gpt-4o-mini
      api_key: os.environ/AZURE_API_KEY
      api_base: os.environ/AZURE_API_BASE
      api_version: "2024-06-01"

  - model_name: gpt-5-nano
    litellm_params:
      model: azure/gpt-5-nano
      api_key: os.environ/AZURE_API_KEY
      api_base: os.environ/AZURE_API_BASE
      api_version: "2024-06-01"

  - model_name: gpt-5-mini
    litellm_params:
      model: azure/gpt-5-mini
      api_key: os.environ/AZURE_API_KEY
      api_base: os.environ/AZURE_API_BASE
      api_version: "2024-06-01"

  # Groq
  - model_name: llama-3.3-70b
    litellm_params:
      model: groq/llama-3.3-70b-versatile
      api_key: os.environ/GROQ_API_KEY

  # Groq — Llama 4 Scout (better tool calling reliability than 3.3, parallel tool use)
  # See: https://console.groq.com/docs/tool-use/overview
  # - model_name: llama-4-scout
  #   litellm_params:
  #     model: groq/meta-llama/llama-4-scout-17b-16e-instruct
  #     api_key: os.environ/GROQ_API_KEY

  # vLLM local models on Modal GPU (uncomment after deploying)
  # Deploy with: VLLM_MODEL=<key> modal deploy scripts/modal_vllm.py --env goat
  # Get the URL from: modal app logs goat-vllm-<key>
  # - model_name: llama-3.1-8b-local
  #   litellm_params:
  #     model: openai/meta-llama/Llama-3.1-8B-Instruct
  #     api_base: https://WORKSPACE--goat-vllm-llama-3.1-8b-serve.modal.run/v1
  #     api_key: "not-needed"

  # - model_name: qwen3-4b-local
  #   litellm_params:
  #     model: openai/Qwen/Qwen3-4B-Instruct-2507
  #     api_base: https://WORKSPACE--goat-vllm-qwen3-4b-serve.modal.run/v1
  #     api_key: "not-needed"

  # - model_name: qwen3-30b-a3b-local
  #   litellm_params:
  #     model: openai/Qwen/Qwen3-30B-A3B-Instruct-2507
  #     api_base: https://WORKSPACE--goat-vllm-qwen3-30b-a3b-serve.modal.run/v1
  #     api_key: "not-needed"

  # - model_name: qwen3-235b-local
  #   litellm_params:
  #     model: openai/Qwen/Qwen3-235B-A22B-Instruct-2507-FP8
  #     api_base: https://WORKSPACE--goat-vllm-qwen3-235b-serve.modal.run/v1
  #     api_key: "not-needed"

  # GPT OSS models on Modal GPU (H100)
  # Deploy with: GPT_OSS_MODEL=<key> modal deploy scripts/modal_gpt_oss.py --env goat
  # - model_name: gpt-oss-20b-local
  #   litellm_params:
  #     model: openai/openai/gpt-oss-20b
  #     api_base: https://WORKSPACE--goat-vllm-gpt-oss-20b-serve.modal.run/v1
  #     api_key: "not-needed"

  # - model_name: gpt-oss-120b-local
  #   litellm_params:
  #     model: openai/openai/gpt-oss-120b
  #     api_base: https://WORKSPACE--goat-vllm-gpt-oss-120b-serve.modal.run/v1
  #     api_key: "not-needed"

  # OpenAI (direct — uncomment if you have a key)
  # - model_name: gpt-4o
  #   litellm_params:
  #     model: openai/gpt-4o
  #     api_key: os.environ/OPENAI_API_KEY

  # Anthropic (uncomment if you have a key)
  # - model_name: claude-sonnet
  #   litellm_params:
  #     model: anthropic/claude-sonnet-4-20250514
  #     api_key: os.environ/ANTHROPIC_API_KEY

  # OpenAI — Embeddings
  - model_name: text-embedding-3-small
    litellm_params:
      model: openai/text-embedding-3-small
      api_key: os.environ/OPENAI_API_KEY

litellm_settings:
  drop_params: true     # silently drop unsupported params instead of erroring
  set_verbose: false
  cache: false
  max_budget: 1000
  budget_duration: 1mo
  success_callback: ["langfuse"]
  failure_callback: ["langfuse"]

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL
  store_model_in_db: true
