# LiteLLM proxy config for Modal deployment (stateless, no database).
# Same models as litellm-config.yaml but without database_url / store_model_in_db.
# Langfuse callbacks work via env vars (LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY, LANGFUSE_HOST).
# See: https://docs.litellm.ai/docs/proxy/configs

model_list:
  # Azure OpenAI (primary chat models)
  - model_name: gpt-4o-mini
    litellm_params:
      model: azure/gpt-4o-mini
      api_key: os.environ/AZURE_API_KEY
      api_base: os.environ/AZURE_API_BASE
      api_version: "2024-06-01"
      base_model: gpt-4o-mini

  - model_name: gpt-5-nano
    litellm_params:
      model: azure/gpt-5-nano
      api_key: os.environ/AZURE_API_KEY
      api_base: os.environ/AZURE_API_BASE
      api_version: "2024-06-01"
      base_model: gpt-4o-mini  # closest known model for cost tracking

  - model_name: gpt-5-mini
    litellm_params:
      model: azure/gpt-5-mini
      api_key: os.environ/AZURE_API_KEY
      api_base: os.environ/AZURE_API_BASE
      api_version: "2024-06-01"
      base_model: gpt-4o-mini  # closest known model for cost tracking

  # Groq
  - model_name: llama-3.3-70b
    litellm_params:
      model: groq/llama-3.3-70b-versatile
      api_key: os.environ/GROQ_API_KEY

  # vLLM local model on Modal GPU (deployed via: modal deploy scripts/modal_vllm.py --env goat)
  - model_name: llama-3.1-8b-local
    litellm_params:
      model: openai/meta-llama/Llama-3.1-8B-Instruct
      api_base: https://phare-goat--goat-vllm-serve.modal.run/v1
      api_key: "not-needed"

  # OpenAI â€” Embeddings
  - model_name: text-embedding-3-small
    litellm_params:
      model: openai/text-embedding-3-small
      api_key: os.environ/OPENAI_API_KEY

litellm_settings:
  drop_params: true
  set_verbose: false
  cache: false
  max_budget: 1000
  budget_duration: 1mo
  success_callback: ["langfuse"]
  failure_callback: ["langfuse"]

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  # No database_url -- runs stateless. Langfuse tracing still works via env vars.
