# LiteLLM proxy config for Modal deployment (stateless, no database).
# Same models as litellm-config.yaml but without database_url / store_model_in_db.
# Langfuse callbacks work via env vars (LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY, LANGFUSE_HOST).
# See: https://docs.litellm.ai/docs/proxy/configs

model_list:
  # Azure OpenAI (primary chat models)
  - model_name: gpt-4o-mini
    litellm_params:
      model: azure/gpt-4o-mini
      api_key: os.environ/AZURE_API_KEY
      api_base: os.environ/AZURE_API_BASE
      api_version: "2024-06-01"
      base_model: gpt-4o-mini

  - model_name: gpt-5-nano
    litellm_params:
      model: azure/gpt-5-nano
      api_key: os.environ/AZURE_API_KEY
      api_base: os.environ/AZURE_API_BASE
      api_version: "2024-06-01"
    model_info:
      base_model: gpt-4o-mini  # cost tracking -- closest known model

  - model_name: gpt-5-mini
    litellm_params:
      model: azure/gpt-5-mini
      api_key: os.environ/AZURE_API_KEY
      api_base: os.environ/AZURE_API_BASE
      api_version: "2024-06-01"
    model_info:
      base_model: gpt-4o-mini  # cost tracking -- closest known model

  # Groq
  - model_name: llama-3.3-70b
    litellm_params:
      model: groq/llama-3.3-70b-versatile
      api_key: os.environ/GROQ_API_KEY

  # vLLM local models on Modal GPU
  # Deploy with: VLLM_MODEL=<key> modal deploy scripts/modal_vllm.py --env goat
  - model_name: llama-3.1-8b-local
    litellm_params:
      model: openai/meta-llama/Llama-3.1-8B-Instruct
      api_base: https://phare-goat--goat-vllm-llama-3-1-8b-serve.modal.run/v1
      api_key: "not-needed"

  - model_name: qwen3-4b-local
    litellm_params:
      model: openai/Qwen/Qwen3-4B-Instruct-2507
      api_base: https://phare-goat--goat-vllm-qwen3-4b-serve.modal.run/v1
      api_key: "not-needed"

  - model_name: qwen3-30b-a3b-local
    litellm_params:
      model: openai/Qwen/Qwen3-30B-A3B-Instruct-2507
      api_base: https://phare-goat--goat-vllm-qwen3-30b-a3b-serve.modal.run/v1
      api_key: "not-needed"

  - model_name: qwen3-235b-local
    litellm_params:
      model: openai/Qwen/Qwen3-235B-A22B-Instruct-2507-FP8
      api_base: https://phare-goat--goat-vllm-qwen3-235b-serve.modal.run/v1
      api_key: "not-needed"

  # GPT OSS models on Modal GPU (H100)
  # Deploy with: GPT_OSS_MODEL=<key> modal deploy scripts/modal_gpt_oss.py --env goat
  - model_name: gpt-oss-20b-local
    litellm_params:
      model: openai/openai/gpt-oss-20b
      api_base: https://phare-goat--goat-vllm-gpt-oss-20b-serve.modal.run/v1
      api_key: "not-needed"

  - model_name: gpt-oss-120b-local
    litellm_params:
      model: openai/openai/gpt-oss-120b
      api_base: https://phare-goat--goat-vllm-gpt-oss-120b-serve.modal.run/v1
      api_key: "not-needed"

  # OpenAI â€” Embeddings
  - model_name: text-embedding-3-small
    litellm_params:
      model: openai/text-embedding-3-small
      api_key: os.environ/OPENAI_API_KEY

litellm_settings:
  drop_params: true
  set_verbose: false
  cache: false
  max_budget: 1000
  budget_duration: 1mo
  success_callback: ["langfuse"]
  failure_callback: ["langfuse"]

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  # No database_url -- runs stateless. Langfuse tracing still works via env vars.
